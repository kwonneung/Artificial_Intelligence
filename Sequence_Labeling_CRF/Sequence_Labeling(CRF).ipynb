{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence Labeling(CRF).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps2BhyiinZo5",
        "outputId": "c67bc601-2f1c-4862-bfb0-e26a1cf98192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdj2kIdRvu_f",
        "outputId": "147223f8-6062-4898-aa25-bb4fafc8a9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grs7Eo0Hslc7",
        "outputId": "251e3886-4e6e-434a-956c-e58bac72812b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "\n",
        "# 파일 경로\n",
        "file_path = \"/gdrive/My Drive/colab/Sequence Labeling/week6/spacing_data.txt\"\n",
        "\n",
        "# \"spacing_data.txt\" 파일을 읽고 lines에 읽은 데이터를 저장\n",
        "with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "    lines = inFile.readlines()\n",
        "\n",
        "# 데이터를 음절로 이루어진 문장과 정답 값으로 나누어 저장\n",
        "datas = []\n",
        "for line in lines:\n",
        "    pieces = line.strip().split(\"\\t\")\n",
        "    eumjeol_sequence, label = pieces[0].split(), pieces[1].split()\n",
        "    datas.append((eumjeol_sequence, label))\n",
        "    \n",
        "number_of_train_datas = int(len(datas)*0.9)\n",
        "\n",
        "train_datas = datas[:number_of_train_datas]\n",
        "test_datas = datas[number_of_train_datas:]\n",
        "          \n",
        "print(\"train_datas 개수 : \" + str(len(train_datas)))          \n",
        "print(\"test_datas 개수 : \" + str(len(test_datas)))\n",
        "\n",
        "for data in train_datas[:5]:\n",
        "  print(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_datas 개수 : 900\n",
            "test_datas 개수 : 100\n",
            "(['약', '속', '장', '소', '인', '신', '라', '호', '텔', '커', '피', '숍', '에', '재', '옥', '이', '먼', '저', '와', '기', '다', '리', '고', '있', '었', '다', '.'], ['B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I'])\n",
            "(['\"', '야', '!', '나', '이', '든', '처', '녀', '가', '옷', '이', '라', '도', '좀', '화', '사', '한', '색', '으', '로', '입', '고', '다', '녀', '라', '.', '회', '색', '에', '다', '검', '정', '바', '바', '리', '가', '뭐', '니', '?', '\"'], ['B', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I'])\n",
            "(['재', '옥', '은', '전', '에', '없', '이', '정', '현', '의', '옷', '차', '림', '을', '탓', '하', '였', '다', '.'], ['B', 'I', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I'])\n",
            "(['\"', '이', '게', '어', '때', '서', '?', '갑', '자', '기', '옷', '얘', '긴', '왜', '하', '니', '?', '선', '보', '일', '일', '있', '니', '?', '\"'], ['B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I'])\n",
            "(['정', '현', '은', '대', '수', '롭', '지', '않', '게', '받', '아', '넘', '겼', '다', '.'], ['B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g0IEVc3sp7j",
        "outputId": "54b58ec9-1fa3-429b-ea45-82babf9b0d89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def sent2feature(eumjeol_sequence):\n",
        "  features = []\n",
        "  sequence_length = len(eumjeol_sequence)\n",
        "  for index in range(sequence_length):\n",
        "      if index == 0:\n",
        "          feature = {\"WORD\":eumjeol_sequence[index], \"F_1\":'^', \"F_2\":eumjeol_sequence[index+1]}\n",
        "      \n",
        "      elif index == sequence_length - 1:\n",
        "          feature = {\"WORD\":eumjeol_sequence[index], \"F_1\":eumjeol_sequence[index-1], \"F_2\":'$'}\n",
        "\n",
        "      else: feature = {\"WORD\":eumjeol_sequence[index], \"F_1\":eumjeol_sequence[index-1], \"F_2\":eumjeol_sequence[index+1]}\n",
        "\n",
        "      features.append(feature)\n",
        "  return features\n",
        "  \n",
        "train_x, train_y = [], []\n",
        "for eumjeol_sequence, label in train_datas:\n",
        "    train_x.append(sent2feature(eumjeol_sequence))\n",
        "    train_y.append(label)\n",
        "\n",
        "test_x, test_y = [], []\n",
        "for eumjeol_sequence, label in test_datas:\n",
        "    test_x.append(sent2feature(eumjeol_sequence))\n",
        "    test_y.append(label)\n",
        "\n",
        "print(train_x[:5],'\\n\\n', train_y[:5])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[{'WORD': '약', 'F_1': '^', 'F_2': '속'}, {'WORD': '속', 'F_1': '약', 'F_2': '장'}, {'WORD': '장', 'F_1': '속', 'F_2': '소'}, {'WORD': '소', 'F_1': '장', 'F_2': '인'}, {'WORD': '인', 'F_1': '소', 'F_2': '신'}, {'WORD': '신', 'F_1': '인', 'F_2': '라'}, {'WORD': '라', 'F_1': '신', 'F_2': '호'}, {'WORD': '호', 'F_1': '라', 'F_2': '텔'}, {'WORD': '텔', 'F_1': '호', 'F_2': '커'}, {'WORD': '커', 'F_1': '텔', 'F_2': '피'}, {'WORD': '피', 'F_1': '커', 'F_2': '숍'}, {'WORD': '숍', 'F_1': '피', 'F_2': '에'}, {'WORD': '에', 'F_1': '숍', 'F_2': '재'}, {'WORD': '재', 'F_1': '에', 'F_2': '옥'}, {'WORD': '옥', 'F_1': '재', 'F_2': '이'}, {'WORD': '이', 'F_1': '옥', 'F_2': '먼'}, {'WORD': '먼', 'F_1': '이', 'F_2': '저'}, {'WORD': '저', 'F_1': '먼', 'F_2': '와'}, {'WORD': '와', 'F_1': '저', 'F_2': '기'}, {'WORD': '기', 'F_1': '와', 'F_2': '다'}, {'WORD': '다', 'F_1': '기', 'F_2': '리'}, {'WORD': '리', 'F_1': '다', 'F_2': '고'}, {'WORD': '고', 'F_1': '리', 'F_2': '있'}, {'WORD': '있', 'F_1': '고', 'F_2': '었'}, {'WORD': '었', 'F_1': '있', 'F_2': '다'}, {'WORD': '다', 'F_1': '었', 'F_2': '.'}, {'WORD': '.', 'F_1': '다', 'F_2': '$'}], [{'WORD': '\"', 'F_1': '^', 'F_2': '야'}, {'WORD': '야', 'F_1': '\"', 'F_2': '!'}, {'WORD': '!', 'F_1': '야', 'F_2': '나'}, {'WORD': '나', 'F_1': '!', 'F_2': '이'}, {'WORD': '이', 'F_1': '나', 'F_2': '든'}, {'WORD': '든', 'F_1': '이', 'F_2': '처'}, {'WORD': '처', 'F_1': '든', 'F_2': '녀'}, {'WORD': '녀', 'F_1': '처', 'F_2': '가'}, {'WORD': '가', 'F_1': '녀', 'F_2': '옷'}, {'WORD': '옷', 'F_1': '가', 'F_2': '이'}, {'WORD': '이', 'F_1': '옷', 'F_2': '라'}, {'WORD': '라', 'F_1': '이', 'F_2': '도'}, {'WORD': '도', 'F_1': '라', 'F_2': '좀'}, {'WORD': '좀', 'F_1': '도', 'F_2': '화'}, {'WORD': '화', 'F_1': '좀', 'F_2': '사'}, {'WORD': '사', 'F_1': '화', 'F_2': '한'}, {'WORD': '한', 'F_1': '사', 'F_2': '색'}, {'WORD': '색', 'F_1': '한', 'F_2': '으'}, {'WORD': '으', 'F_1': '색', 'F_2': '로'}, {'WORD': '로', 'F_1': '으', 'F_2': '입'}, {'WORD': '입', 'F_1': '로', 'F_2': '고'}, {'WORD': '고', 'F_1': '입', 'F_2': '다'}, {'WORD': '다', 'F_1': '고', 'F_2': '녀'}, {'WORD': '녀', 'F_1': '다', 'F_2': '라'}, {'WORD': '라', 'F_1': '녀', 'F_2': '.'}, {'WORD': '.', 'F_1': '라', 'F_2': '회'}, {'WORD': '회', 'F_1': '.', 'F_2': '색'}, {'WORD': '색', 'F_1': '회', 'F_2': '에'}, {'WORD': '에', 'F_1': '색', 'F_2': '다'}, {'WORD': '다', 'F_1': '에', 'F_2': '검'}, {'WORD': '검', 'F_1': '다', 'F_2': '정'}, {'WORD': '정', 'F_1': '검', 'F_2': '바'}, {'WORD': '바', 'F_1': '정', 'F_2': '바'}, {'WORD': '바', 'F_1': '바', 'F_2': '리'}, {'WORD': '리', 'F_1': '바', 'F_2': '가'}, {'WORD': '가', 'F_1': '리', 'F_2': '뭐'}, {'WORD': '뭐', 'F_1': '가', 'F_2': '니'}, {'WORD': '니', 'F_1': '뭐', 'F_2': '?'}, {'WORD': '?', 'F_1': '니', 'F_2': '\"'}, {'WORD': '\"', 'F_1': '?', 'F_2': '$'}], [{'WORD': '재', 'F_1': '^', 'F_2': '옥'}, {'WORD': '옥', 'F_1': '재', 'F_2': '은'}, {'WORD': '은', 'F_1': '옥', 'F_2': '전'}, {'WORD': '전', 'F_1': '은', 'F_2': '에'}, {'WORD': '에', 'F_1': '전', 'F_2': '없'}, {'WORD': '없', 'F_1': '에', 'F_2': '이'}, {'WORD': '이', 'F_1': '없', 'F_2': '정'}, {'WORD': '정', 'F_1': '이', 'F_2': '현'}, {'WORD': '현', 'F_1': '정', 'F_2': '의'}, {'WORD': '의', 'F_1': '현', 'F_2': '옷'}, {'WORD': '옷', 'F_1': '의', 'F_2': '차'}, {'WORD': '차', 'F_1': '옷', 'F_2': '림'}, {'WORD': '림', 'F_1': '차', 'F_2': '을'}, {'WORD': '을', 'F_1': '림', 'F_2': '탓'}, {'WORD': '탓', 'F_1': '을', 'F_2': '하'}, {'WORD': '하', 'F_1': '탓', 'F_2': '였'}, {'WORD': '였', 'F_1': '하', 'F_2': '다'}, {'WORD': '다', 'F_1': '였', 'F_2': '.'}, {'WORD': '.', 'F_1': '다', 'F_2': '$'}], [{'WORD': '\"', 'F_1': '^', 'F_2': '이'}, {'WORD': '이', 'F_1': '\"', 'F_2': '게'}, {'WORD': '게', 'F_1': '이', 'F_2': '어'}, {'WORD': '어', 'F_1': '게', 'F_2': '때'}, {'WORD': '때', 'F_1': '어', 'F_2': '서'}, {'WORD': '서', 'F_1': '때', 'F_2': '?'}, {'WORD': '?', 'F_1': '서', 'F_2': '갑'}, {'WORD': '갑', 'F_1': '?', 'F_2': '자'}, {'WORD': '자', 'F_1': '갑', 'F_2': '기'}, {'WORD': '기', 'F_1': '자', 'F_2': '옷'}, {'WORD': '옷', 'F_1': '기', 'F_2': '얘'}, {'WORD': '얘', 'F_1': '옷', 'F_2': '긴'}, {'WORD': '긴', 'F_1': '얘', 'F_2': '왜'}, {'WORD': '왜', 'F_1': '긴', 'F_2': '하'}, {'WORD': '하', 'F_1': '왜', 'F_2': '니'}, {'WORD': '니', 'F_1': '하', 'F_2': '?'}, {'WORD': '?', 'F_1': '니', 'F_2': '선'}, {'WORD': '선', 'F_1': '?', 'F_2': '보'}, {'WORD': '보', 'F_1': '선', 'F_2': '일'}, {'WORD': '일', 'F_1': '보', 'F_2': '일'}, {'WORD': '일', 'F_1': '일', 'F_2': '있'}, {'WORD': '있', 'F_1': '일', 'F_2': '니'}, {'WORD': '니', 'F_1': '있', 'F_2': '?'}, {'WORD': '?', 'F_1': '니', 'F_2': '\"'}, {'WORD': '\"', 'F_1': '?', 'F_2': '$'}], [{'WORD': '정', 'F_1': '^', 'F_2': '현'}, {'WORD': '현', 'F_1': '정', 'F_2': '은'}, {'WORD': '은', 'F_1': '현', 'F_2': '대'}, {'WORD': '대', 'F_1': '은', 'F_2': '수'}, {'WORD': '수', 'F_1': '대', 'F_2': '롭'}, {'WORD': '롭', 'F_1': '수', 'F_2': '지'}, {'WORD': '지', 'F_1': '롭', 'F_2': '않'}, {'WORD': '않', 'F_1': '지', 'F_2': '게'}, {'WORD': '게', 'F_1': '않', 'F_2': '받'}, {'WORD': '받', 'F_1': '게', 'F_2': '아'}, {'WORD': '아', 'F_1': '받', 'F_2': '넘'}, {'WORD': '넘', 'F_1': '아', 'F_2': '겼'}, {'WORD': '겼', 'F_1': '넘', 'F_2': '다'}, {'WORD': '다', 'F_1': '겼', 'F_2': '.'}, {'WORD': '.', 'F_1': '다', 'F_2': '$'}]] \n",
            "\n",
            " [['B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I'], ['B', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I'], ['B', 'I', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I'], ['B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I'], ['B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvAzDpBLssqg",
        "outputId": "98d5d228-89a4-43ca-dbe0-38813bcc1776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs')\n",
        "crf.fit(train_x, train_y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=None,\n",
              "    averaging=None, c=None, c1=None, c2=None, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=None,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu_qjoNkswe_",
        "outputId": "e168396f-5332-4d3f-a137-c6ce706fea08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def show_predict_result(test_datas, predict):\n",
        "  for index_1 in range(len(test_datas)):\n",
        "      eumjeol_sequence, correct_labels = test_datas[index_1]\n",
        "      predict_labels = predict[index_1]\n",
        "     \n",
        "      correct_sentence, predict_sentence = \"\", \"\"\n",
        "      for index_2 in range(len(eumjeol_sequence)):\n",
        "          if(index_2 == 0):\n",
        "              correct_sentence += eumjeol_sequence[index_2]\n",
        "              predict_sentence += eumjeol_sequence[index_2]\n",
        "              continue\n",
        "\n",
        "          if(correct_labels[index_2] == \"B\"):\n",
        "              correct_sentence += \" \"\n",
        "          correct_sentence += eumjeol_sequence[index_2]\n",
        "\n",
        "          if (predict_labels[index_2] == \"B\"):\n",
        "              predict_sentence += \" \"\n",
        "          predict_sentence += eumjeol_sequence[index_2]\n",
        "\n",
        "      print(\"정답 문장 : \" + correct_sentence)\n",
        "      print(\"출력 문장 : \" + predict_sentence)\n",
        "      print()\n",
        "\n",
        "predict = crf.predict(test_x)\n",
        "\n",
        "print(\"Accuracy score : \" + str(metrics.flat_accuracy_score(test_y, predict)))\n",
        "print(\"\\n10개의 데이터에 대한 모델 출력과 실제 정답 비교\\n\")\n",
        "\n",
        "show_predict_result(test_datas[:10], predict[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score : 0.8994658527279664\n",
            "\n",
            "10개의 데이터에 대한 모델 출력과 실제 정답 비교\n",
            "\n",
            "정답 문장 : 1914- 18년의 전쟁은 인류를 통합시킨 최초의 공통분모였다.\n",
            "출력 문장 : 19 14- 18년의 전쟁은 인류를 통합시킨 최초의 공통 분모였다.\n",
            "\n",
            "정답 문장 : 하지만 이 전쟁은 죽음을 통해 인류를 통합시켰다.\n",
            "출력 문장 : 하지만이 전쟁은 죽음을 통해 인류를 통합시켰다.\n",
            "\n",
            "정답 문장 : 사라예보에서 한 세르비아인이 쏜 총 한발이 합스부르크가의 계승자를 죽였다.\n",
            "출력 문장 : 사라 예보에서 한세르 비아인이 쏜총한 발이 합스부르크가의 계승자를 죽였다.\n",
            "\n",
            "정답 문장 : 이 암살행위는 국지적인 민족주의들과 세계적인 제국주의들이 충돌하는 분쟁지역에서 저질러졌다.\n",
            "출력 문장 : 이암 살행 위는 국지적인 민족주의 들과 세계적인 제국주의 들이 충돌하는 분쟁 지역에서 저질러졌다.\n",
            "\n",
            "정답 문장 : 오토만제국의 점진적인 해체는 민족주의의 독기를 발산하는 동시에 오스트리아, 헝가리와 독일, 영국, 프랑스의 탐욕을 자극했다.\n",
            "출력 문장 : 오토만 제국의 점진적인해체는 민족주의의 독기를 발산하는 동시에 오스 트리아, 헝가리와 독일, 영국, 프랑스의 탐욕을 자극했다.\n",
            "\n",
            "정답 문장 : 이렇게 해서 발칸 반도의 한 외진 장소에서 벌어진 국지적인 테러 행위는 일련의 긴박한 반응을 불러 일으키면서 전 유럽에 영향을 미쳤을 뿐만 아니라 이번에는 아시아와 아프리카 식민지들, 일본, 그리고 이어서 미국과 멕시코까지 끌어들였다.\n",
            "출력 문장 : 이렇게 해서 발칸 반도의 한외진 장소에서 벌어 진국지적인테 러행 위는 일련의 긴박한 반응을 불러일으키면서 전유럽에 영향을 미쳤을 뿐만 아니라이 번에는 아시아와 아프리 카식민지들, 일본, 그리고 이어서 미국과 멕시코까지 끌어들였다.\n",
            "\n",
            "정답 문장 : 전쟁의 물결이 지구상의 모든 대양으로 밀려드는 동안 캐나다인들과 미국인들, 오스트레일리아인들, 세네갈인들, 알제리인들, 모로코인들, 안남(安南)인들은 연합군 깃발을 휘날리며 유럽전선에서 싸웠다.\n",
            "출력 문장 : 전쟁의 물결이 지구상의 모든 대양으로 밀려드는 동안 캐나다인들과 미국인들, 오스트레 일리 아인들, 세네 갈인들, 알 제리인들, 모로 코인들, 안남(安南)인들은 연합군 깃발을 휘날리며 유럽 전선에서 싸웠다.\n",
            "\n",
            "정답 문장 : 앞서 살펴본 것처럼, 세계대전의 원인이 된 것은 대립관계에 있던 유럽 제국주의의 구심적 회귀였다.\n",
            "출력 문장 : 앞서 살펴본 것처럼, 세계대전의 원인이된 것은 대립 관계에 있던 유럽 제국주의 의구심적회귀였다.\n",
            "\n",
            "정답 문장 : 그리고 세계대전을 촉발시킨 것은 주요 제국주의들과 소수 민족주의들 간의 상호작용이었다.\n",
            "출력 문장 : 그리고 세계대전을 촉발 시킨 것은 주요 제국주의 들과 소수 민족주의 들간의 상호작용이었다.\n",
            "\n",
            "정답 문장 : 또한 세계대전의 빌미를 제공한 것은 격화된 민족주의들이었다.\n",
            "출력 문장 : 또한 세계대전의 빌미를 제공한 것은 격화된 민족주의 들이었다.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}