{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.5 64-bit ('base': conda)",
   "display_name": "Python 3.6.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "89cfcc2101ac9601337bf699cf426e47927e374f8b5aab845ae7d30939bbb508"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm   #진행도를 가시화 시켜주는 것!\n",
    "\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    file = open(os.path.join(data_dir, file_path),encoding='utf-8')\n",
    "\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for line in tqdm(file.readlines()):\n",
    "        text, label = line.strip().split('\\t')\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2686/2686 [00:00<00:00, 448822.78it/s]\n100%|██████████| 300/300 [00:00<00:00, 150189.93it/s]\n"
    }
   ],
   "source": [
    "train_X, train_Y = load_data('sentiment_train.txt')\n",
    "test_X, test_Y= load_data('sentiment_test.txt')\n",
    "\n",
    "train_X_pos, train_pos = [(train_X[i],train_Y[i]) for i in range(len(train_X)) if train_Y[i] == '<P>'], [train_X[i] for i in range(len(train_X)) if train_Y[i] == '<P>']\n",
    "train_X_neg, train_neg = [(train_X[i],train_Y[i]) for i in range(len(train_X)) if train_Y[i] == '<N>'], [train_X[i] for i in range(len(train_X)) if train_Y[i] == '<N>']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       ㄴ가  ㄴ가요   ㄴ걸   ㄴ고   ㄴ다  ㄴ다고  ㄴ다구  ㄴ다기에  ㄴ다네  ㄴ다는  ...   흐뭇   흔들  흔들리  \\\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n...   ...  ...  ...  ...  ...  ...  ...   ...  ...  ...  ...  ...  ...  ...   \n1337  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n1338  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n1339  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n1340  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n1341  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n\n       흔하   흘리   흡족   흰색   힘겹   힘들  힘빠져서  \n0     0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n1     0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n2     0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n3     0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n4     0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n...   ...  ...  ...  ...  ...  ...   ...  \n1337  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n1338  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n1339  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n1340  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n1341  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n\n[1342 rows x 1733 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ㄴ가</th>\n      <th>ㄴ가요</th>\n      <th>ㄴ걸</th>\n      <th>ㄴ고</th>\n      <th>ㄴ다</th>\n      <th>ㄴ다고</th>\n      <th>ㄴ다구</th>\n      <th>ㄴ다기에</th>\n      <th>ㄴ다네</th>\n      <th>ㄴ다는</th>\n      <th>...</th>\n      <th>흐뭇</th>\n      <th>흔들</th>\n      <th>흔들리</th>\n      <th>흔하</th>\n      <th>흘리</th>\n      <th>흡족</th>\n      <th>흰색</th>\n      <th>힘겹</th>\n      <th>힘들</th>\n      <th>힘빠져서</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1337</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1338</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1339</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1340</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1341</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1342 rows × 1733 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "pos_vectorizer = TfidfVectorizer(min_df=1, analyzer = 'word')\n",
    "feature_train_Xpos = pos_vectorizer.fit_transform(train_pos)\n",
    "# , smooth_idf=False, sublinear_tf=False\n",
    "neg_vectorizer = TfidfVectorizer(min_df=1, analyzer = 'word')\n",
    "feature_train_Xneg = neg_vectorizer.fit_transform(train_neg)\n",
    "\n",
    "df_pos = pd.DataFrame(feature_train_Xpos.toarray(),columns=list(pos_vectorizer.get_feature_names()))\n",
    "df_neg = pd.DataFrame(feature_train_Xneg.toarray(),columns=list(neg_vectorizer.get_feature_names()))\n",
    "df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ㄴ데', 'ㅂ니다', '가격', '감동', '감사', '강력', '강추', '강추합니다', '걱정', '고맙', '고요', '괜찮', '구매', '구요', '구입', '그리고', '기능', '기분', '기사', '까지', '깔끔', '깨끗', '너무', '너무너무', '네요', '는데', '다고', '다른', '대비', '대우', '돌아가', '드럼', '드리', '디자인', '마음', '만족', '많이', '매우', '면서', '모두', '무엇', '물건', '배달', '배송', '배송도', '번창', '보다', '비하', '빠르', '빨래', '빨리', '사용', '사용법', '상품', '색상', '생각', '서비스', '설명', '설치', '성능', '세탁', '세탁기', '소음', '수거', '스럽', '습니다', '아서', '아요', '아저씨', '아주', '아직', '암튼', '어서', '어요', '없이', '에서', '에요', '예쁘', '완전', '용량', '으로', '으시', '이불', '이쁘', '저렴', '적극', '적당', '정도', '정말', '제품', '조용', '좋아하', '지만', '최고', '추천', '친절', '크기', '편하', '품질', '하구'] \n ['ㄴ다고', 'ㄴ데', 'ㄴ지', 'ㅂ니다', '가격', '감사', '걸리', '괜찮', '구매', '구요', '구입', '그것', '그냥', '그래도', '그러', '그런데', '그렇', '근데', '기다리', '기분', '기사', '까지', '깔끔', '너무', '네요', '느리', '는데', '니까', '다고', '다는', '다른', '다만', '다시', '대비', '더군요', '드럼', '들어가', '디자인', '라고', '만족', '많이', '모르', '문제', '물건', '배송', '배송이', '보다', '비하', '빠르', '빨래', '빨리', '사용', '상품', '생각', '서비스', '설치', '세탁', '세탁기', '소리', '소음', '스럽', '습니다', '시간', '실망', '심하', '아니', '아서', '아야', '아요', '아주', '아직', '암튼', '어서', '어야', '어요', '에서', '연락', '완전', '용량', '으로', '으면', '은데', '이것', '이나', '이렇', '이불', '일주일', '저렴', '전화', '정도', '정말', '제품', '조금', '주문', '지만', '짜증', '친절', '크기', '탈수', '하지만']\n"
    }
   ],
   "source": [
    "pos_voc = df_pos.sum().sort_values(ascending=False)[:100].index\n",
    "neg_voc = df_neg.sum().sort_values(ascending=False)[:100].index\n",
    "print(sorted(pos_voc),'\\n',sorted(neg_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "142 \n ['ㄴ다고', 'ㄴ데', 'ㄴ지', 'ㅂ니다', '가격', '감동', '감사', '강력', '강추', '강추합니다', '걱정', '걸리', '고맙', '고요', '괜찮', '구매', '구요', '구입', '그것', '그냥', '그래도', '그러', '그런데', '그렇', '그리고', '근데', '기능', '기다리', '기분', '기사', '까지', '깔끔', '깨끗', '너무', '너무너무', '네요', '느리', '는데', '니까', '다고', '다는', '다른', '다만', '다시', '대비', '대우', '더군요', '돌아가', '드럼', '드리', '들어가', '디자인', '라고', '마음', '만족', '많이', '매우', '면서', '모두', '모르', '무엇', '문제', '물건', '배달', '배송', '배송도', '배송이', '번창', '보다', '비하', '빠르', '빨래', '빨리', '사용', '사용법', '상품', '색상', '생각', '서비스', '설명', '설치', '성능', '세탁', '세탁기', '소리', '소음', '수거', '스럽', '습니다', '시간', '실망', '심하', '아니', '아서', '아야', '아요', '아저씨', '아주', '아직', '암튼', '어서', '어야', '어요', '없이', '에서', '에요', '연락', '예쁘', '완전', '용량', '으로', '으면', '으시', '은데', '이것', '이나', '이렇', '이불', '이쁘', '일주일', '저렴', '적극', '적당', '전화', '정도', '정말', '제품', '조금', '조용', '좋아하', '주문', '지만', '짜증', '최고', '추천', '친절', '크기', '탈수', '편하', '품질', '하구', '하지만']\n"
    }
   ],
   "source": [
    "voca = sorted(set(list(pos_voc)+list(neg_voc)))\n",
    "# stop = list(set(pos_voc).intersection(list(neg_voc)))\n",
    "print(len(voca),'\\n',voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "vectorizer= CountVectorizer()\n",
    "feature_train_X= vectorizer.fit(voca).transform(train_X)\n",
    "\n",
    "#Naive Bayes train\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(feature_train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<N>', '<P>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<P>', '<N>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<N>', '<P>', '<P>', '<P>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<P>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<P>', '<P>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<P>', '<N>', '<P>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<N>', '<P>', '<P>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<N>', '<P>', '<P>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<P>', '<N>', '<N>', '<N>', '<N>', '<P>', '<P>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<N>', '<P>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<P>', '<P>', '<N>', '<P>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>', '<N>', '<N>', '<P>', '<N>']\n"
    }
   ],
   "source": [
    "feature_test_X = vectorizer.transform(test_X)\n",
    "prediction = classifier.predict(feature_test_X).tolist()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.770\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: %.3f'% accuracy_score(test_Y,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}