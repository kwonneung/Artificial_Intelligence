{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitgnleevirtualenv71e06e81b1d340fa8a981c124af6b997",
   "display_name": "Python 3.6.8 64-bit ('gnlee': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "89cfcc2101ac9601337bf699cf426e47927e374f8b5aab845ae7d30939bbb508"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm #데이터 로드 시각화\n",
    "\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    file = open(os.path.join(data_dir, file_path), encoding='UTF-8')\n",
    "\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for line in tqdm(file.readlines()):\n",
    "        text, label = line.strip().split('\\t')\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "from math import log\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t,docs):\n",
    "    N = 1\n",
    "    df = 0\n",
    "    # for doc in docs:\n",
    "    df += t in docs\n",
    "    return log(N/df+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2686/2686 [00:00<00:00, 673113.49it/s]\n100%|██████████| 300/300 [00:00<00:00, 301820.87it/s]\n\n 1099 1733\n\n"
    }
   ],
   "source": [
    "train_X, train_Y = load_data('sentiment_train.txt')\n",
    "test_X, test_Y= load_data('sentiment_test.txt')\n",
    "\n",
    "pos_corpus = ' '.join([train_X[i] for i in range(len(train_X)) if train_Y[i] == '<P>'])\n",
    "neg_corpus = ' '.join([train_X[i] for i in range(len(train_X)) if train_Y[i] == '<N>'])\n",
    "\n",
    "pos_bow = list(set(pos_corpus.split(' ')))\n",
    "pos_bow = [x for x in pos_bow if len(x)>1]\n",
    "neg_bow = list(set(neg_corpus.split(' ')))\n",
    "neg_bow = [x for x in neg_bow if len(x)>1]\n",
    "\n",
    "print('\\n\\n',len(pos_bow), len(neg_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pos, tfidf_neg = [], []\n",
    "for word in pos_bow:\n",
    "    tfidf_pos.append(tf(word,pos_corpus)*idf(word,pos_corpus))\n",
    "\n",
    "for word in neg_bow:\n",
    "    tfidf_neg.append(tf(word,neg_corpus)*idf(word,neg_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['가격', 'ㅂ니다', '배송', '친절', '아요', '만족', '너무', '네요', '세탁', '습니다', '대비',\n       '기사', '는데', '설치', '어요', '아서', '저렴', '세탁기', '어서', '괜찮', '빠르', '구요', '감사',\n       '제품', '디자인', '성능', '사용', '빨래', '보다', '아주', '지만', '정말', '배송도', '깔끔',\n       '비하', '상품', '추천', '으로', '드럼', '품질', '에서', '깨끗', '생각', '기능', '강추', '드리',\n       '설명', '구입', '물건', '소음', '까지', '고요', '마음', '서비스', '많이', '대우', '이쁘',\n       '아저씨', '빨리', '그리', '다른', '으시', '완전', '스럽', '다고', '매우', '너무너무', '구매',\n       '무엇', '면서', '크기', '기분', '아직', '고맙', '고장', '모르', '걱정', '암튼', '없이', 'ㄴ데',\n       '라구', '그리고', '용량', '배달', '편하', '에요', '모두', '번창', '적극', '조용', '수거', '이불',\n       '예쁘', '소리', '좋아하', '특히', '시간', '감동', '더라구', '다는'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_postfidf = pd.DataFrame(tfidf_pos,index=pos_bow)\n",
    "df_negtfidf = pd.DataFrame(tfidf_neg, index=neg_bow)\n",
    "\n",
    "pos_voc = df_postfidf.sort_values(by=0, ascending=False)[:100].index\n",
    "neg_voc = df_negtfidf.sort_values(by=0, ascending=False)[:100].index\n",
    "\n",
    "pos_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['배송', '세탁', '는데', '습니다', '아서', 'ㅂ니다', '네요', '세탁기', '어서', '지만', '너무',\n       '어요', '아요', '전화', '설치', '보다', '소음', '가격', '생각', '소리', '빨래', '기사', '다고',\n       '친절', '다는', 'ㄴ다', '만족', '상품', '물건', '그래', '제품', '다른', '괜찮', '에서', '조금',\n       '다리', '으로', '기다리', '니까', '구요', '사용', '그런', '주문', '걸리', '아니', '많이', 'ㄴ데',\n       '느리', '정말', '그래도', '시간', '라고', '구매', '아야', '탈수', '근데', '빠르', '그러', '그렇',\n       '군요', '비스', '송이', '으면', '서비스', 'ㄴ지', '더군', '드럼', '더니', '짜증', '어야', '까지',\n       '디자인', '하지', '이불', '은데', '그런데', '문제', '배송이', '판매', '이나', '스럽', '구입',\n       '정도', '연락', '하지만', '심하', 'ㄴ다고', '대우', '더군요', '기분', '라도', '저렴', '그냥',\n       '다가', '아무', '아주', '주일', '다만', '대비', '판매자'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "neg_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "148 \n\n ['ㄴ다', 'ㄴ다고', 'ㄴ데', 'ㄴ지', 'ㅂ니다', '가격', '감동', '감사', '강추', '걱정', '걸리', '고맙', '고요', '고장', '괜찮', '구매', '구요', '구입', '군요', '그냥', '그래', '그래도', '그러', '그런', '그런데', '그렇', '그리', '그리고', '근데', '기능', '기다리', '기분', '기사', '까지', '깔끔', '깨끗', '너무', '너무너무', '네요', '느리', '는데', '니까', '다가', '다고', '다는', '다른', '다리', '다만', '대비', '대우', '더군', '더군요', '더니', '더라구', '드럼', '드리', '디자인', '라고', '라구', '라도', '마음', '만족', '많이', '매우', '면서', '모두', '모르', '무엇', '문제', '물건', '배달', '배송', '배송도', '배송이', '번창', '보다', '비스', '비하', '빠르', '빨래', '빨리', '사용', '상품', '생각', '서비스', '설명', '설치', '성능', '세탁', '세탁기', '소리', '소음', '송이', '수거', '스럽', '습니다', '시간', '심하', '아니', '아무', '아서', '아야', '아요', '아저씨', '아주', '아직', '암튼', '어서', '어야', '어요', '없이', '에서', '에요', '연락', '예쁘', '완전', '용량', '으로', '으면', '으시', '은데', '이나', '이불', '이쁘', '저렴', '적극', '전화', '정도', '정말', '제품', '조금', '조용', '좋아하', '주문', '주일', '지만', '짜증', '추천', '친절', '크기', '탈수', '특히', '판매', '판매자', '편하', '품질', '하지', '하지만']\n"
    }
   ],
   "source": [
    "voca = sorted(set(list(pos_voc)+list(neg_voc)))\n",
    "print(len(voca),'\\n\\n',voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['제품 너무 너무',\n '드럼 세탁기 ㅂ니다',\n '너무 아요',\n '조금 지만',\n '너무너무',\n '완전 친절 저씨',\n '빨리 많 ㄴ다 는데 가격 저렴 아주 아요',\n '정말 기다리 어서 어요',\n '',\n '기다리 어야']"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "ntrain_X = []\n",
    "for text in train_X:\n",
    "    for word in text.split(' '):\n",
    "        if word not in voca:\n",
    "            text = text.replace(word,'')\n",
    "    ntrain_X.append(text)\n",
    "\n",
    "for i in range(len(ntrain_X)):\n",
    "    ntrain_X[i]= ' '.join(ntrain_X[i].split())\n",
    "\n",
    "ntrain_X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['대우 세탁기 구입 ㄴ다',\n '그런데 배송 느리',\n '가격 대비 세탁기',\n '더군요',\n '배송 기사 너무 게 구입 어요',\n '제품 구매 는데 자 설명 기분 않답니다',\n '적극 추천 아요',\n '보다 구요',\n '대우 세탁기 에요',\n '주문 는데 아직 네요']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "ntest_X = []\n",
    "for text in test_X:\n",
    "    for word in text.split(' '):\n",
    "        if word not in voca:\n",
    "            text = text.replace(word,'')\n",
    "    ntest_X.append(text)\n",
    "\n",
    "for i in range(len(ntest_X)):\n",
    "    ntest_X[i]= ' '.join(ntest_X[i].split())\n",
    "ntest_X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Le = LabelEncoder()  \n",
    "\n",
    "train_Y= Le.fit_transform(train_Y)\n",
    "test_Y= Le.fit_transform(test_Y)\n",
    "\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2484, 147)\n"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, analyzer = 'word')\n",
    "doc2vec = vectorizer.fit(voca).transform(ntrain_X)\n",
    "train = pd.DataFrame(doc2vec.toarray(),index=['doc'+ str(x) for x in range(1,len(ntrain_X)+1)]) \n",
    "train['Y'] = train_Y\n",
    "\n",
    "train1 = train[train.Y == 1] \n",
    "train0 = train[train.Y == 0] \n",
    "\n",
    "train1_1 = train1[train1.sum(axis=1) > 1]\n",
    "train0_0 = train0[train0.sum(axis=1) > 0]\n",
    "\n",
    "train = train[train.index.isin(train1_1.index|train0_0.index)]\n",
    "x_train= train.iloc[:,:-2]\n",
    "y_train= train.iloc[:,-1]\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(283, 147)\n"
    }
   ],
   "source": [
    "doc2vec = vectorizer.transform(ntest_X)\n",
    "\n",
    "test = pd.DataFrame(doc2vec.toarray(), index=['doc'+ str(x) for x in range(1,len(ntest_X)+1)])\n",
    "test['Y'] = test_Y\n",
    "\n",
    "test1 = test[test.Y == 1] \n",
    "test0 = test[test.Y== 0] \n",
    "\n",
    "test1_1 = test1[test1.sum(axis=1) > 1]\n",
    "test0_0 = test0[test0.sum(axis=1) > 0]\n",
    "\n",
    "test = test[test.index.isin(test1_1.index|test0_0.index)]\n",
    "x_test= test.iloc[:,:-2]\n",
    "y_test= test.iloc[:,-1]\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n"
    }
   ],
   "source": [
    "feature_train_X= x_train.to_numpy()\n",
    "#Naive Bayes train\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(feature_train_X, y_train)\n",
    "\n",
    "feature_test_X = x_test.to_numpy()\n",
    "prediction = classifier.predict(feature_test_X).tolist()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.760\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: %.3f'% accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}